{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optimum-quanto\n",
    "!pip install accelerate\n",
    "!git clone https://github.com/huggingface/diffusers.git\n",
    "!pip install -e \"diffusers/.[torch]\"\n",
    "!pip install -e \"diffusers/.[flax]\"\n",
    "!git -C diffusers/ pull\n",
    "!pip install transformers --upgrade \n",
    "\n",
    "import torch # necessary to check the device\n",
    "# identify which device is used (cuda = GPU, cpu = CPU only, mps = Mac)\n",
    "device: str = 'cuda' if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "if device == 'cpu':\n",
    "    !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "elif device == 'cuda':\n",
    "    !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "elif device == 'mps':\n",
    "    !pip3 install torch torchvision torchaudio\n",
    "else:\n",
    "    print(\"device unknown\")\n",
    "# exception: cu124 necessary for google colab no matter if T4 GPU enabled or CPU only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import accelerate\n",
    "\n",
    "from optimum.quanto import freeze, qfloat8, quantize\n",
    "\n",
    "from diffusers.models.transformers.transformer_flux import FluxTransformer2DModel\n",
    "from diffusers.pipelines.flux.pipeline_flux import FluxPipeline\n",
    "\n",
    "from transformers import T5EncoderModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define loading and saving path for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = '../../models/text-to-image/flux.1-schnell' # saving path\n",
    "\n",
    "model = \"black-forest-labs/FLUX.1-schnell\" # official model flux1.-schnell from Blackforest (not quantized)\n",
    "model_tr = \"https://huggingface.co/Kijai/flux-fp8/blob/main/flux1-schnell-fp8.safetensors\" # quantized transformer from Hugginface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load and quantize transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = FluxTransformer2DModel.from_single_file(model_tr, \n",
    "                                                        torch_dtype=torch.bfloat16,\n",
    "                                                        cache_dir = cache_dir\n",
    ")\n",
    "quantize(transformer, weights=qfloat8)\n",
    "freeze(transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load and quantize text_encoder_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder_2 = T5EncoderModel.from_pretrained(model,\n",
    "                                                subfolder=\"text_encoder_2\",\n",
    "                                                torch_dtype=torch.bfloat16,\n",
    "                                                cache_dir=cache_dir\n",
    ")\n",
    "quantize(text_encoder_2, weights=qfloat8)\n",
    "freeze(text_encoder_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up pipe line with main model and the two quantized models (transformer & text_encoder_2). When running on cuda (GPU) there are some more \"tricks\" to lower the memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = FluxPipeline.from_pretrained(model,\n",
    "                                    transformer=None,\n",
    "                                    text_encoder_2=None,\n",
    "                                    torch_dtype=torch.bfloat16\n",
    ")\n",
    "pipe.transformer = transformer\n",
    "pipe.text_encoder_2 = text_encoder_2\n",
    "pipe.to(torch.device(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For cuda (GPU) use ONLY to save some VRAM on GPU to get the code running with VRAM < 16 GB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == 'cuda':\n",
    "    pipe.enable_model_cpu_offload() # offloads modules to CPU on a submodule level (rather than model level)\n",
    "    # pipe.enable_sequential_cpu_offload() # when using non-quantized versions to make it run with VRAM 4-32 GB\n",
    "    # pipe.vae.enable_slicing() # when using non-quantized versions to make it run with VRAM 4-32 GB\n",
    "    # pipe.vae.enable_tiling() # when using non-quantized versions to make it run with VRAM 4-32 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define parameters for the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Ancient soldier with a sword and a shield. Behind there are horses. In the background there is a mountain with snow.\"\n",
    "height, width = 128, 128\n",
    "num_inference_steps = 4  # number of iterations, 4 gives decent results and should be considered as minimum; people on hugging face and git hub ~15-50 iterations\n",
    "generator = torch.Generator(\"cpu\").manual_seed(12345) # set seed for repeatable results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    guidance_scale=0.0, # must be 0.0 for flux1.-schnell, may be 3.5 for flux1.-dev but up to 7.0 --> higher guidance scale forces the model to keep closer to the prompt at the expense of image quality\n",
    "    height=height,\n",
    "    width=width,\n",
    "    #output_type=\"pil\",\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    max_sequence_length=128, #256 is max for flux1.-schnell; maximum sequence length to use with the prompt\n",
    "    generator=generator\n",
    ").images[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.save(f\"figs/Kijai_qt-qte2_{num_inference_steps}_{height}_{width}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
